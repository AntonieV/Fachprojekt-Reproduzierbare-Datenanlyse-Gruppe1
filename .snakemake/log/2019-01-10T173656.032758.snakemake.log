Building DAG of jobs...
Creating conda environment envs/sleuth.yaml...
Downloading remote packages.
Environment for envs/sleuth.yaml created (location: .snakemake/conda/be680c08)
Using shell: /usr/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	sleuth
	2

[Thu Jan 10 17:38:13 2019]
rule sleuth:
    input: kallisto/a, kallisto/b, kallisto/c, kallisto/d, table_for_reads.csv
    output: sleuth/significant_transcripts.csv
    jobid: 1

Activating conda environment: /home/sattle00/Fachprojekt/fprdg1/.snakemake/conda/be680c08
[Thu Jan 10 17:38:18 2019]
Finished job 1.
1 of 2 steps (50%) done

[Thu Jan 10 17:38:18 2019]
localrule all:
    input: sleuth/significant_transcripts.csv
    jobid: 0

[Thu Jan 10 17:38:18 2019]
Finished job 0.
2 of 2 steps (100%) done
Complete log: /home/sattle00/Fachprojekt/fprdg1/.snakemake/log/2019-01-10T173656.032758.snakemake.log
